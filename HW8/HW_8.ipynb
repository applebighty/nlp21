{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/main/HW8/HW_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccZrwCiYNoLI"
      },
      "source": [
        "# Download Data and Get Set Up\n",
        "\n",
        "### In this homework, we'll be using a larger vocabulary of case-sensitive GloVe word embedding vectors - this will take a few minutes to download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcVLcPSgNoyM"
      },
      "source": [
        "!wget https://people.ischool.berkeley.edu/~jongillick/nlp/devQA.pkl\n",
        "!wget https://people.ischool.berkeley.edu/~jongillick/nlp/trainQA.pkl\n",
        "!wget https://people.ischool.berkeley.edu/~dbamman/glove.1M.300d.cased.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg8XKI2qZbNn"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import sys, json, re, time\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxu_P-mDZbqX"
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in tqdm(enumerate(file)):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab\n",
        "\n",
        "print(\"Loading word embeddings. This will take a minute or two...\")\n",
        "\n",
        "embs, vocab = read_embeddings(\"glove.1M.300d.cased.txt\", vocab_size=1000000)\n",
        "\n",
        "def make_reverse_vocab(vocab):\n",
        "    # Flip the keys and values in a dict.\n",
        "    vocab['UNKNOWN'] = 1\n",
        "    vocab['PAD'] = 0\n",
        "    rv = {}\n",
        "    for k in vocab.keys():\n",
        "        rv[vocab[k]] = k\n",
        "    return rv\n",
        "\n",
        "reverse_vocab = make_reverse_vocab(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zrcydenUqlR"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj2_ze4cTyZf"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75EPRzc_U-vp"
      },
      "source": [
        "# Load QAPair Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW1hbnl8TNx1"
      },
      "source": [
        "class QAPair:\n",
        "\tdef __init__(self, idd, number_annotators_for_answer, is_impossible, question, context, answer_text, answer_character_start):\n",
        "\t\tself.idd=idd\n",
        "\t\t# SQUAD has multiple answers per question; this is the number of annotators for the majority answer\n",
        "\t\t# let's only use answers that have at least two annotators\n",
        "\t\tself.number_annotators_for_answer=number_annotators_for_answer\n",
        "\n",
        "\t\t# SQUAD 2.0 has questions that cannot be answered from the context.  Is this question impossible to answer?\n",
        "\t\tself.is_impossible=is_impossible\n",
        "\n",
        "\t\t# The question\n",
        "\t\tself.question=question\n",
        "\n",
        "\t\t# The answer text within the context\n",
        "\t\tself.answer_text=answer_text\n",
        "\n",
        "\t\t# The character offset of the answer within the context\n",
        "\t\tself.answer_start=answer_character_start\n",
        "\n",
        "\t\t# The paragraph to try to find the answer in\n",
        "\t\tself.context=context\n",
        "\n",
        "\t\tspacy_context_tokens=nlp(self.context)\n",
        "\t\t# This is the tokenized context\n",
        "\t\tself.context_tokens=[tok.text for tok in spacy_context_tokens]\n",
        "\n",
        "\t\t# This is the tokenized question\n",
        "\t\tself.question_tokens=[tok.text for tok in nlp(self.question)]\n",
        "\n",
        "\t\tself.context_pos=[tok.tag_ for tok in spacy_context_tokens]\n",
        "\t\tself.context_ner_iob=[\"%s-%s\" % (tok.ent_iob_, tok.ent_type_) for tok in spacy_context_tokens]\n",
        "\n",
        "    # The index of the token in self.context_tokens that corresponds to the answer start.  The answer is the sequence\n",
        "\t\t# self.context_tokens[self.answer_start_token:self.answer_end_token]\n",
        "\t\tself.answer_start_token=None\n",
        "\n",
        "\t\t# The index of the token in self.context_tokens that corresponds to the answer end.  The answer is the sequence\n",
        "\t\t# self.context_tokens[self.answer_start_token:self.answer_end_token]\n",
        "\t\tself.answer_end_token=None\n",
        "\n",
        "\t\tif not self.is_impossible:\n",
        "\n",
        "\t\t\tself.answer_end=self.answer_start+len(self.answer_text)\n",
        "\n",
        "\t\t\t# get index of answer start and end *token* in tokenized context\n",
        "\t\t\tcurrentCharacter=0\n",
        "\n",
        "\t\t\ttoken_start=None\n",
        "\t\t\ttoken_end=None\n",
        "\n",
        "\t\t\tfor idx, word in enumerate(spacy_context_tokens):\n",
        "\n",
        "\t\t\t\tws=0\n",
        "\t\t\t\tend=word.idx+len(word)\n",
        "\n",
        "\t\t\t\tif idx < len(spacy_context_tokens)-1:\n",
        "\t\t\t\t\tnextStart=spacy_context_tokens[idx+1].idx\n",
        "\t\t\t\t\tws=nextStart-end\n",
        "\n",
        "\t\t\t\tif self.answer_start == currentCharacter:\n",
        "\t\t\t\t\tself.answer_start_token=idx\n",
        "\n",
        "\t\t\t\tif self.answer_end == currentCharacter + len(word):\n",
        "\t\t\t\t\tself.answer_end_token=idx + 1\n",
        "\n",
        "\t\t\t\tcurrentCharacter+=len(word) + ws\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tself.answer_end=None\n",
        "\n",
        "\t\tif self.answer_start_token is not None and self.answer_end_token is not None:\n",
        "\t\t\tassert re.sub(\" \", \"\", ' '.join(self.context_tokens[self.answer_start_token:self.answer_end_token])) == re.sub(\" \", \"\", self.answer_text)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\n",
        "\t\treturn '\\t'.join([str(x) for x in [self.idd, self.number_annotators_for_answer, self.is_impossible, self.question, self.context, self.answer_start, self.answer_end, self.answer_start_token, self.answer_end_token]])\n",
        "\n",
        "def read(filename, limit=100000000):\n",
        "\tquestion_answer_pairs=[]\n",
        "\n",
        "\twith open(filename) as file:\n",
        "\t\tdata=json.load(file)\n",
        "\t\tfor datum in tqdm(data[\"data\"][:limit]):\n",
        "\t\t\ttitle=datum[\"title\"]\n",
        "\t\t\tfor paragraph in datum[\"paragraphs\"]:\n",
        "\t\t\t\tcontext=paragraph[\"context\"]\n",
        "\t\t\t\tfor qa in paragraph[\"qas\"]:\n",
        "\n",
        "\t\t\t\t\tquestion=qa[\"question\"]\n",
        "\t\t\t\t\tidd=qa[\"id\"]\n",
        "\t\t\t\t\tis_impossible=qa[\"is_impossible\"]\n",
        "\t\t\t\t\tanswers=qa[\"answers\"]\n",
        "\n",
        "\t\t\t\t\tmajority_answer=None\n",
        "\n",
        "\t\t\t\t\tanswer_counts=Counter()\n",
        "\n",
        "\t\t\t\t\tif not is_impossible:\n",
        "\t\t\t\t\t\tfor answer in answers:\n",
        "\t\t\t\t\t\t\ttext=answer[\"text\"]\n",
        "\t\t\t\t\t\t\tanswer_start=answer[\"answer_start\"]\n",
        "\t\t\t\t\t\t\tanswer_counts[(text, answer_start)]+=1\n",
        "\n",
        "\t\t\t\t\t\tfor k, v in answer_counts.most_common():\n",
        "\t\t\t\t\t\t\tlength=len(text)\n",
        "\t\t\t\t\t\t\tassert text == context[answer_start:answer_start+length]\n",
        "\t\t\t\t\t\t\tquestion_answer_pairs.append(QAPair(idd, v, is_impossible, question, context, text, answer_start))\n",
        "\t\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tquestion_answer_pairs.append(QAPair(idd, v, is_impossible, question, context, None, None))\n",
        "\n",
        "\treturn question_answer_pairs\n",
        "\n",
        "with open('devQA.pkl', 'rb') as f:\n",
        "    devQA= pickle.load(f)\n",
        "    \n",
        "with open('trainQA.pkl', 'rb') as f:\n",
        "    trainQA= pickle.load(f)\n",
        "\n",
        "max_paragraph_length = 150\n",
        "max_question_length = 30\n",
        "\n",
        "train_qa_pairs=[]\n",
        "for qa in trainQA:\n",
        "  if not qa.is_impossible and qa.answer_start_token is not None and qa.answer_end_token is not None and len(qa.context_tokens) < max_paragraph_length and len(qa.question_tokens) < max_question_length:\n",
        "    train_qa_pairs.append(qa)\n",
        "\n",
        "dev_qa_pairs=[]\n",
        "for qa in devQA:\n",
        "  if not qa.is_impossible and qa.answer_start_token is not None and qa.answer_end_token is not None and len(qa.context_tokens) < max_paragraph_length and len(qa.question_tokens) < max_question_length:\n",
        "    dev_qa_pairs.append(qa)\n",
        "\n",
        "train_qa_pairs = sorted(train_qa_pairs, key=lambda x: len(x.context_tokens)) \n",
        "dev_qa_pairs = sorted(dev_qa_pairs, key=lambda x: len(x.context_tokens)) \n",
        "\n",
        "for tqap in train_qa_pairs:\n",
        "  tqap.answer_end_token = tqap.answer_end_token-1\n",
        "\n",
        "for dqap in dev_qa_pairs:\n",
        "  dqap.answer_end_token = dqap.answer_end_token-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kTTo_6VVUu"
      },
      "source": [
        "Now that all the data is loaded, let's take a look at it. We'll be working with question & answer pairs from the SQuAD dataset using a class called QAPair. In SQuAD, questions are paired with a paragraph of text called the \"context\", and the answer comes in the form of a span of text that is highlighted in that paragraph. We will be training a model to predict the start and end points of that span. Let's take a look at what's in a QAPair datapoint. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1mq1UnVm_SR"
      },
      "source": [
        "qa_pair = dev_qa_pairs[11]\n",
        "print(\"Context paragraph:\")\n",
        "print(f\"{qa_pair.context}\\n\")\n",
        "print(\"Question:\")\n",
        "print(f\"{qa_pair.question}\\n\")\n",
        "print(\"Answer:\")\n",
        "print(f\"{qa_pair.answer_text}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib2K5ypRfWDQ"
      },
      "source": [
        "Next, let's look more closely at the way this text data is structured and at the features that we will be using in our QA model.  The question and the context have been tokenized and POS and NER features have been automatically tagged for the context (using the spacy library). The answer label has been provided via `answer_start_token` and `answer_end_token`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNXsuZrPzskE"
      },
      "source": [
        "print(\"Tokenized Context\")\n",
        "print(f\"{qa_pair.context_tokens}\\n\")\n",
        "print(\"Context POS tags\")\n",
        "print(f\"{qa_pair.context_pos}\\n\")\n",
        "print(\"Context NER tags\")\n",
        "print(f\"{qa_pair.context_ner_iob}\\n\")\n",
        "print(\"Answer Text:\")\n",
        "print(f\"{qa_pair.answer_text}\\n\")\n",
        "print(\"Answer Start Token:\")\n",
        "print(f\"{qa_pair.answer_start_token}\\n\")\n",
        "print(\"Answer End Token:\")\n",
        "print(f\"{qa_pair.answer_end_token}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us0LlAyeoUgg"
      },
      "source": [
        "# Deliverable 1: Adding Features for Part of Speech and Named Entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmlQ3v5--2Eo"
      },
      "source": [
        "`pos_tag_list` and `ner_tag_list` contain the sets of all possible POS and NER features that the words in a QAPair are tagged with. When we pre-processes our data for training using `get_batches`, these strings will be converted to integer-valued ID's using the `pos_vocab` and `ner_vocab` dictionaries defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUe11ChDuLWK"
      },
      "source": [
        "pos_tag_list = ['$',\"''\",',','-LRB-','-RRB-','.',':','ADD',\n",
        "'AFX','CC','CD','DT','EX','FW','HYPH','IN','JJ','JJR',\n",
        "'JJS','LS','MD','NFP','NN','NNP','NNPS','NNS','PDT','POS',\n",
        "'PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB',\n",
        "'VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB',\n",
        "'XX','_SP','``']\n",
        "\n",
        "ner_tag_list = ['B-CARDINAL', 'B-DATE', 'B-EVENT', 'B-FAC',\n",
        " 'B-GPE', 'B-LANGUAGE', 'B-LAW', 'B-LOC', 'B-MONEY',\n",
        " 'B-NORP', 'B-ORDINAL', 'B-ORG', 'B-PERCENT', 'B-PERSON',\n",
        " 'B-PRODUCT', 'B-QUANTITY', 'B-TIME', 'B-WORK_OF_ART',\n",
        " 'I-CARDINAL', 'I-DATE', 'I-EVENT', 'I-FAC', 'I-GPE',\n",
        " 'I-LAW', 'I-LOC', 'I-MONEY', 'I-NORP', 'I-ORG',\n",
        " 'I-PERCENT', 'I-PERSON', 'I-PRODUCT', 'I-QUANTITY',\n",
        " 'I-TIME', 'I-WORK_OF_ART', 'O-']\n",
        "\n",
        "pos_vocab = {}\n",
        "ner_vocab = {}\n",
        "\n",
        "for i, t in enumerate(pos_tag_list): \n",
        "  pos_vocab[t] = i+1 # 0 means PAD\n",
        "\n",
        "for i, t in enumerate(ner_tag_list): \n",
        "  ner_vocab[t] = i+1 # 0 means PAD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p5KYbH9cZUO"
      },
      "source": [
        "class NeuralQA(nn.Module):\n",
        "\n",
        "  def __init__(self, pretrained_embeddings, pos_tag_list, ner_tag_list):\n",
        "    super(NeuralQA, self).__init__()\n",
        "    self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "    self.vocab_size, self.embedding_dim=pretrained_embeddings.shape\n",
        "    self.lstm_dim=128\n",
        "\n",
        "    self.pos_tag_list=pos_tag_list\n",
        "    self.ner_tag_list=ner_tag_list\n",
        "    self.pos_dim_embedding_dim=16\n",
        "    self.ner_dim_embedding_dim=16\n",
        "    \n",
        "    self.question_lstm = nn.LSTM(self.embedding_dim, self.lstm_dim, bidirectional=True, batch_first=True, dropout=0.3, num_layers=2)\n",
        "    self.context_lstm = nn.LSTM(2*self.embedding_dim+self.pos_dim_embedding_dim+self.ner_dim_embedding_dim, self.lstm_dim, bidirectional=True, batch_first=True, dropout=0.3, num_layers=3)\n",
        "\n",
        "    # p attends over the output of the question LSTM\n",
        "    self.p_attention = nn.MultiheadAttention(2*self.lstm_dim, 1)\n",
        "\n",
        "    # p_to_q attends from the context *embeddings* to the question *embeddings*\n",
        "    self.p_to_q_attention = nn.MultiheadAttention(self.embedding_dim, 1)\n",
        "    \n",
        "    self.W_start=nn.Linear(2*self.lstm_dim,2*self.lstm_dim)\n",
        "    self.W_end=nn.Linear(2*self.lstm_dim,2*self.lstm_dim)\n",
        "    self.drop_layer_030 = nn.Dropout(p=0.3)\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    self.pos_embeddings = ...\n",
        "    self.ner_embeddings = ...\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, question_ids, context_ids, pos_ids, ner_ids):\n",
        "\n",
        "    batch_size, _=question_ids.shape\n",
        "    question_query=torch.FloatTensor(np.ones((batch_size,1,2*self.lstm_dim))).to(device)\n",
        "\n",
        "    question_embeds=self.word_embeddings(question_ids)\n",
        "    context_embeds=self.word_embeddings(context_ids)\n",
        "\n",
        "    question_embeds=self.drop_layer_030(question_embeds)\n",
        "    context_embeds=self.drop_layer_030(context_embeds)\n",
        "\n",
        "    question_embeds=question_embeds.transpose(0,1)\n",
        "    context_embeds=context_embeds.transpose(0,1)\n",
        "    \n",
        "    question_lstm_out, _=self.question_lstm(question_embeds)\n",
        "\n",
        "    p_attn_output, _ = self.p_attention(question_query, question_lstm_out, question_lstm_out)\n",
        "    p_q_attn_output, _ = self.p_to_q_attention(context_embeds, question_embeds, question_embeds)\n",
        "\n",
        "    p_q_attn_output=p_q_attn_output.transpose(0,1) # max_seq_length x batch_size x self.embedding_dim -> batch_size x max_seq_length x self.embedding_dim\n",
        "\n",
        "    context_embeds=context_embeds.transpose(0,1)\n",
        "    question_embeds=question_embeds.transpose(0,1)\n",
        "\n",
        "    context_representation=torch.cat((context_embeds, p_q_attn_output), dim=2)\n",
        "\n",
        "    # YOUR CODE GOES HERE \n",
        "    pos_embeds=...\n",
        "    ner_embeds=...\n",
        "    context_representation=...\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    context_lstm_out, _=self.context_lstm(context_representation)\n",
        "    context_lstm_out=context_lstm_out.transpose(1,2) # batch_size x max_seq_length x 200 -> batch_size x 200 x max_seq_length\n",
        "\n",
        "    p_start=self.W_start(p_attn_output) \n",
        "    p_start = torch.matmul(p_start, context_lstm_out)\n",
        "\n",
        "    p_end=self.W_end(p_attn_output) \n",
        "    p_end = torch.matmul(p_end, context_lstm_out)\n",
        "\n",
        "    p_start=p_start.squeeze(1)\n",
        "    p_end=p_end.squeeze(1)\n",
        "\n",
        "    p_start=torch.exp(p_start)\n",
        "    p_start=p_start/(torch.sum(p_start, dim=1).unsqueeze(-1))\n",
        "    \n",
        "    p_end=torch.exp(p_end)\n",
        "    p_end=p_end/(torch.sum(p_end, dim=1).unsqueeze(-1))\n",
        "\n",
        "    return p_start, p_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gGTEZN5KXgS"
      },
      "source": [
        "def get_ids(tokens, vocab):\n",
        "  ids = []\n",
        "  for t in tokens:\n",
        "    if t in vocab:\n",
        "      ids.append(vocab[t])\n",
        "    else:\n",
        "      ids.append(1) #UNKNOWN\n",
        "  return ids\n",
        "\n",
        "def get_tag_ids(tokens, tag_vocab):\n",
        "  ids = []\n",
        "  for t in tokens:\n",
        "    ids.append(tag_vocab[t])\n",
        "  return ids\n",
        "\n",
        "def get_batches(qa_pairs, batch_size=32):\n",
        "    batches_x=[]\n",
        "    batches_y=[]\n",
        "\n",
        "    question_ids = []\n",
        "    context_ids = []\n",
        "    pos_ids = []\n",
        "    ner_ids = []\n",
        "    exact_match_features = []\n",
        "    answer_start_indexes = []\n",
        "    answer_end_indexes = []\n",
        "\n",
        "    for i in range(len(qa_pairs)):\n",
        "      qa_pair = qa_pairs[i]\n",
        "\n",
        "      question_ids.append(get_ids(qa_pair.question_tokens, vocab))\n",
        "      context_ids.append(get_ids(qa_pair.context_tokens, vocab))\n",
        "\n",
        "      pos_ids.append(get_tag_ids(qa_pair.context_pos, pos_vocab))\n",
        "      ner_ids.append(get_tag_ids(qa_pair.context_ner_iob, ner_vocab))\n",
        "\n",
        "      answer_start_indexes.append(qa_pair.answer_start_token)\n",
        "      answer_end_indexes.append(qa_pair.answer_end_token)\n",
        "\n",
        "    for i in range(0, len(qa_pairs), batch_size):\n",
        "      batch_x = {}; batch_y = {}\n",
        "\n",
        "      batch_question_ids = question_ids[i:i+batch_size]\n",
        "      max_q_len = max([len(qids) for qids in batch_question_ids])\n",
        "      for qids in batch_question_ids:\n",
        "        qids.extend([PAD_INDEX] * (max_q_len-len(qids)))\n",
        "      batch_x['question_ids'] = batch_question_ids\n",
        "\n",
        "      batch_context_ids = context_ids[i:i+batch_size]\n",
        "      max_c_len = max([len(cids) for cids in batch_context_ids])\n",
        "      for cids in batch_context_ids:\n",
        "        cids.extend([PAD_INDEX] * (max_c_len-len(cids)))\n",
        "      batch_x['context_ids'] = batch_context_ids\n",
        "\n",
        "      batch_pos_ids = pos_ids[i:i+batch_size]\n",
        "      for pids in batch_pos_ids:\n",
        "        pids.extend([PAD_INDEX] * (max_c_len-len(pids)))\n",
        "      batch_x['pos_ids'] = batch_pos_ids\n",
        "\n",
        "      batch_ner_ids = ner_ids[i:i+batch_size]\n",
        "      for nids in batch_ner_ids:\n",
        "        nids.extend([PAD_INDEX] * (max_c_len-len(nids)))\n",
        "      batch_x['ner_ids'] = batch_ner_ids\n",
        "\n",
        "      batch_answer_start_indexes = answer_start_indexes[i:i+batch_size]\n",
        "      batch_answer_end_indexes = answer_end_indexes[i:i+batch_size]\n",
        "      batch_y['answer_start_indexes'] = batch_answer_start_indexes\n",
        "      batch_y['answer_end_indexes'] = batch_answer_end_indexes\n",
        "\n",
        "      batches_x.append(batch_x)\n",
        "      batches_y.append(batch_y)\n",
        "\n",
        "    return batches_x, batches_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MmwlGcs05y4"
      },
      "source": [
        "def evaluate(model, dev_batches_x, dev_batches_y):\n",
        "  batch_losses = []\n",
        "  model.eval()\n",
        "  correct=total=0.\n",
        "  for i in range(len(dev_batches_x)):\n",
        "    batch_x = dev_batches_x[i]\n",
        "    batch_y = dev_batches_y[i]\n",
        "\n",
        "    question_ids = torch.LongTensor(batch_x['question_ids']).to(device)\n",
        "    context_ids = torch.LongTensor(batch_x['context_ids']).to(device)\n",
        "    pos_ids = torch.LongTensor(batch_x['pos_ids']).to(device)\n",
        "    ner_ids = torch.LongTensor(batch_x['ner_ids']).to(device)\n",
        "\n",
        "    question_mask = (question_ids > 0).int()\n",
        "    context_mask = (context_ids > 0).int()\n",
        "\n",
        "    answer_start_indexes = batch_y['answer_start_indexes']\n",
        "    answer_end_indexes = batch_y['answer_end_indexes']\n",
        "\n",
        "    batch_size, context_size = context_ids.shape\n",
        "\n",
        "    start_labels = torch.FloatTensor(np.zeros((batch_size, context_size))).to(device)\n",
        "    end_labels = torch.FloatTensor(np.zeros((batch_size, context_size))).to(device)\n",
        "\n",
        "    for index, label in enumerate(answer_start_indexes):\n",
        "      start_labels[index, label] = 1\n",
        "\n",
        "    for index, label in enumerate(answer_end_indexes):\n",
        "      end_labels[index, label] = 1\n",
        "\n",
        "    start_preds, end_preds = model(question_ids, context_ids, pos_ids, ner_ids)\n",
        "\n",
        "    start_preds=start_preds*context_mask\n",
        "    end_preds=end_preds*context_mask\n",
        "\n",
        "    start_max=torch.argmax(start_preds, dim=1)\n",
        "    end_max=torch.argmax(end_preds, dim=1)\n",
        "\n",
        "    start_loss = (cross_entropy_loss(start_preds, start_labels) * context_mask).mean()\n",
        "    end_loss = (cross_entropy_loss(end_preds, end_labels) * context_mask).mean()\n",
        "\n",
        "    loss = start_loss + end_loss\n",
        "\n",
        "    batch_losses.append(float(loss.detach().cpu()))\n",
        "\n",
        "    for index, label in enumerate(answer_start_indexes):\n",
        "      start=label\n",
        "      end=answer_end_indexes[index]\n",
        "\n",
        "      bestProb=0\n",
        "      bestPair=None,None\n",
        "\n",
        "      start_index=start_max[index]\n",
        "      w_idx=context_ids[index][start_index]\n",
        "      \n",
        "      if w_idx > 0:\n",
        "        # end token can be the same as the start token\n",
        "        for offset in range(0,15):\n",
        "          end_index=start_index+offset\n",
        "\n",
        "          if end_index >= context_size:\n",
        "            continue\n",
        "\n",
        "          w_idx2=context_ids[index][end_index]\n",
        "\n",
        "          if w_idx2 <= 0:\n",
        "            continue\n",
        "\n",
        "          prob=start_preds[index][start_index]*end_preds[index][end_index]\n",
        "          if prob > bestProb:\n",
        "            bestProb=prob\n",
        "            bestPair=start_index, end_index\n",
        "      \n",
        "      total+=1\n",
        "\n",
        "      if start == bestPair[0] and end == bestPair[1]:\n",
        "        correct+=1\n",
        "\n",
        "  eval_loss = np.mean(batch_losses)\n",
        "  exact_match=correct/total\n",
        "  return eval_loss, exact_match"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZSJUCOR083_"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAsd17P-0_BN"
      },
      "source": [
        "After completing Deliverable 1.1 and 1.2, you will be able to train your network to answer questions.  We evaluate the model using an accuracy metric called \"Exact Match\". This metric tests whether the span predicted by the model exactly matches the labeled answer.  This means that both the `answer_start` and `answer_end` tokens need to be correctly predicted in order to count as a correct answer.\n",
        "\n",
        "Because Question Answering is a challenging task that requires reasoning not just about the syntax and semantics of text, but about any entities (e.g. The Empire State Building or California) that might appear in the data, we need a fairly large amount of data to train this model, and we use a larger, case-sensitive vocabulary of pre-trained GloVe vectors.  The training code might take 10 minutes or more to run, so keep that in mind when working on your assignment! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8NIUsVUZFDC"
      },
      "source": [
        "model=NeuralQA(embs, pos_tag_list, ner_tag_list).to(device)\n",
        "cross_entropy_loss = nn.BCELoss()\n",
        "optimizer = torch.optim.Adamax(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjSfXk6fNBO5"
      },
      "source": [
        "train_batches_x, train_batches_y = get_batches(train_qa_pairs, batch_size=128)\n",
        "dev_batches_x, dev_batches_y = get_batches(dev_qa_pairs, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52EPfRJdM2Ey"
      },
      "source": [
        "train_loss = None\n",
        "model.eval()\n",
        "dev_loss, exact_match = evaluate(model, dev_batches_x, dev_batches_y)\n",
        "print(f\"Dev Loss: {dev_loss}, Accuracy(Exact Match): {exact_match}\")\n",
        "t0 = time.time()\n",
        "for epoch in range(10):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  model.train()\n",
        "  train_batches_x, train_batches_y = shuffle(train_batches_x, train_batches_y)\n",
        "  epoch_losses = []\n",
        "  for i in tqdm(range(len(train_batches_x))):\n",
        "    batch_x = train_batches_x[i]\n",
        "    batch_y = train_batches_y[i]\n",
        "\n",
        "    question_ids = torch.LongTensor(batch_x['question_ids']).to(device)\n",
        "    context_ids = torch.LongTensor(batch_x['context_ids']).to(device)\n",
        "    pos_ids = torch.LongTensor(batch_x['pos_ids']).to(device)\n",
        "    ner_ids = torch.LongTensor(batch_x['ner_ids']).to(device)\n",
        "\n",
        "    question_mask = (question_ids > 0).int()\n",
        "    context_mask = (context_ids > 0).int()\n",
        "\n",
        "    answer_start_indexes = batch_y['answer_start_indexes']\n",
        "    answer_end_indexes = batch_y['answer_end_indexes']\n",
        "\n",
        "    batch_size, context_size = context_ids.shape\n",
        "\n",
        "    start_labels = torch.FloatTensor(np.zeros((batch_size, context_size))).to(device)\n",
        "    end_labels = torch.FloatTensor(np.zeros((batch_size, context_size))).to(device)\n",
        "\n",
        "    for index, label in enumerate(answer_start_indexes):\n",
        "      start_labels[index, label] = 1\n",
        "\n",
        "    for index, label in enumerate(answer_end_indexes):\n",
        "      end_labels[index, label] = 1\n",
        "\n",
        "    start_preds, end_preds = model(question_ids, context_ids, pos_ids, ner_ids)\n",
        "    start_loss = (cross_entropy_loss(start_preds, start_labels) * context_mask).mean()\n",
        "    end_loss = (cross_entropy_loss(end_preds, end_labels) * context_mask).mean()\n",
        "\n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    epoch_losses.append(float(loss.detach().cpu()))\n",
        "  train_loss = np.mean(epoch_losses)\n",
        "  model.eval()\n",
        "  dev_loss, exact_match = evaluate(model, dev_batches_x, dev_batches_y)\n",
        "  training_time = time.time()-t0\n",
        "  secs = int(training_time % 60)\n",
        "  mins = int(training_time / 60)\n",
        "  print(f\"Train Loss: {train_loss}, Dev Loss: {dev_loss}, Accuracy(Exact Match): {exact_match}, training time: {mins}:{secs}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF8QWmxngVY9"
      },
      "source": [
        "# Deliverable 2: Answering Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS9B1whaiuq7"
      },
      "source": [
        "def get_top_spans(start_probs, end_probs, context_length):\n",
        "  # \"we choose the best span from token i to token i′ such that i≤i′≤i+15 and Pstart(i)×Pend(i′) is maximized\"\n",
        "  span_pairs = {}\n",
        "  for i in range(context_length):\n",
        "    for j in range(context_length):\n",
        "      if j >= i and j <= i+15:\n",
        "        prod = float(start_probs[i]) * float(end_probs[j])\n",
        "        key = f\"{i},{j}\"      \n",
        "        span_pairs[key] = prod\n",
        "  top_spans = {k: v for k, v in sorted(span_pairs.items(), key=lambda item: item[1], reverse=True)}\n",
        "  return top_spans\n",
        "\n",
        "def ids_to_readable_string(ids):\n",
        "  return ' '.join([reverse_vocab[id] for id in ids])\n",
        "\n",
        "def get_answer(context_ids, start_index, end_index):\n",
        "  return ids_to_readable_string(context_ids[start_index:end_index+1])\n",
        "\n",
        "def get_top_answers(context_ids, context_length, start_probs, end_probs, k):\n",
        "  answers = []\n",
        "  top_spans = get_top_spans(start_probs, end_probs, context_length)\n",
        "  for i in range(k):\n",
        "    start_index, end_index = [int(s) for s in list(top_spans.keys())[i].split(',')]\n",
        "    answers.append(get_answer(context_ids, start_index, end_index))\n",
        "  return answers\n",
        "\n",
        "def answer_question(your_paragraph, your_question):\n",
        "  your_qa_pair = QAPair(idd='123456789',number_annotators_for_answer=None,is_impossible=True,\n",
        "        question=your_question,context=your_paragraph,answer_text=None,answer_character_start=None)\n",
        "  your_batch = get_batches([your_qa_pair], batch_size=1)\n",
        "  batches_x, batches_y = your_batch\n",
        "  batch_x = batches_x[0]\n",
        "  question_ids = torch.LongTensor(batch_x['question_ids']).to(device)\n",
        "  context_ids = torch.LongTensor(batch_x['context_ids']).to(device)\n",
        "  pos_ids = torch.LongTensor(batch_x['pos_ids']).to(device)\n",
        "  ner_ids = torch.LongTensor(batch_x['ner_ids']).to(device)\n",
        "  question_mask = (question_ids > 0).int()\n",
        "  context_mask = (context_ids > 0).int()\n",
        "  start_preds, end_preds = model(question_ids, context_ids, pos_ids, ner_ids)\n",
        "  start_preds = start_preds*context_mask\n",
        "  end_preds = end_preds*context_mask\n",
        "  batch_index = 0\n",
        "  readable_q = ids_to_readable_string(batch_x['question_ids'][batch_index])\n",
        "  readable_par = ids_to_readable_string(batch_x['context_ids'][batch_index])\n",
        "  print(\"\\nContext:\")\n",
        "  print(f\"{readable_par}\\n\")\n",
        "  print(\"Question:\")\n",
        "  print(f\"{readable_q}\\n\")\n",
        "  print(\"Top Predicted Answers:\")\n",
        "  top_answers = get_top_answers(batch_x['context_ids'][batch_index], int(context_mask.sum()), start_preds[batch_index], end_preds[batch_index], k=10)\n",
        "  for answer in top_answers:\n",
        "    print(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEbznnHzmtjY"
      },
      "source": [
        "Now that we have a trained model, let's try answering some questions of your own. Choose a paragraph from any page on Wikipedia (or write your own if you are feeling adventurous), copy that paragraph into the Colab (make sure it's less than 150 words, since we restricted our training data to short paragraphs to make the training time more manageable), and write a question that can be answered by highlighting a span from that paragraph. Try to find one paragraph/question pair that the model is able to answer, and another paragraph/question pair that the model is not able to answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toktsQ0gmsk3"
      },
      "source": [
        "# YOUR CODE GOES HERE\n",
        "your_paragraph_1 = ...\n",
        "your_question_1 = ...\n",
        "# END OF YOUR CODE\n",
        "\n",
        "answer_question(your_paragraph_1, your_question_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGYiekKTwYJZ"
      },
      "source": [
        "# YOUR CODE GOES HERE\n",
        "your_paragraph_2 = ...\n",
        "your_question_2 = ...\n",
        "# END OF YOUR CODE\n",
        "\n",
        "answer_question(your_paragraph_2, your_question_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLugdw3yzA98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}