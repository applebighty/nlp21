{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/main/HW7/HW_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQNh_TKTxh5"
      },
      "source": [
        "import json\n",
        "import gensim\n",
        "import sys\n",
        "import gensim.downloader as api\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import nltk\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxDT4OEa6G_Q"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qDRQRrUzuaW"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htiw0DFSFCq"
      },
      "source": [
        "#Let's install qwikidata, which we will use to qurey the wikidata triples\n",
        "!pip install qwikidata\n",
        "#Let's install pywikibot, which we will use to query wikipedia articles\n",
        "!pip install pywikibot\n",
        "!export PYWIKIBOT_NO_USER_CONFIG=1\n",
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1Bh7YZMjVT"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/glove.6B.50d.50K.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/dev_dataset.csv\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/train_dataset.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C21-6PV4xqQf"
      },
      "source": [
        "user_config = '''\n",
        "mylang = 'en'\n",
        "family = 'wikipedia'\n",
        "username = 'ExampleBot'\n",
        "'''\n",
        "with open(\"./user-config.py\", \"w\") as f:\n",
        "  f.write(user_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNXmHiNw6jRl"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bVEu6IP6j2q"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfvHlfmOLpf2"
      },
      "source": [
        "# Deliverable 1: Distant Supervision\n",
        "\n",
        "In this component, we will query wikipedia articles and structured triples of information.  We will align the triples with text via the algorithm in SLP Chapter 17 Figure 17.9.  We will be doing this for a small set of Wikipedia articles and relationship types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlyn7pSv5953"
      },
      "source": [
        "First, let's download the text of the Wikipedia articles we are interested in.  For the purpose of Deliverable 1, this is 50 Alternative Hip Hop artists.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCco2We-OII"
      },
      "source": [
        "\n",
        "import pywikibot\n",
        "import wikipedia\n",
        "from pywikibot import pagegenerators\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import requests\n",
        "site = pywikibot.Site()\n",
        "\n",
        "#This line grabs all the wikipedia articles for a given category\n",
        "cat = pywikibot.Category(site,'Category:Alternative hip hop musicians')\n",
        "#Now, we will use pywikibot to grab all wikipedia pages linked to this category\n",
        "gen = pagegenerators.CategorizedPageGenerator(cat)\n",
        "\n",
        "\n",
        "documents = {}\n",
        "#For this homework, we will be stopping once 10 articles are queried\n",
        "counter = 0\n",
        "\n",
        "#iterates through all the pages\n",
        "for page in tqdm(gen):\n",
        "    #grabs the title as plaintext\n",
        "    title = page.title(with_ns=False)\n",
        "    \n",
        "    #If errors, we skip over the entity \n",
        "    try:\n",
        "      #grabs the wikipedia page text\n",
        "      text = wikipedia.page(title)\n",
        "      documents[title] = text.content\n",
        "      counter += 1 \n",
        "    except:\n",
        "      continue\n",
        "    if counter > 50:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzHkOspjd_sv"
      },
      "source": [
        "Let's look at an example of one of the returned wikipedia pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyeGR9heCNg"
      },
      "source": [
        "documents['WebsterX']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIpUptt_7wjM"
      },
      "source": [
        "Because the Wikidata corpus is incredibly large, we will use a series of sparql queries to get relevant triples for our corpus.  We will return all relevant triples for alternative hip hop artists and filter out later.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more details on Wikidata sparql queries if interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejT-QCy4pjG"
      },
      "source": [
        "'''\n",
        "Now we will define the 5 queries we will be using for the distant supervision.\n",
        "\n",
        "We are interested in artists' date of birth, place of birth, school attended,\n",
        "start of musician career, and band name.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more information on how to structure queries like this, if interested\n",
        "'''\n",
        "\n",
        "q1 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?dob \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P569 ?dob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "q2 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?pobLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P19 ?pob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q3 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?schoolLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P69 ?school.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q4 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?start \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P2031 ?start.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q5 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?memberLabel\n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P463 ?member.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "queries = [q1, q2, q3, q4, q5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCeP63glSFE4"
      },
      "source": [
        "'''\n",
        "Next, let's query wikidata for the triples we are interested in \n",
        "\n",
        "This cell makes a sparql query to wikidata to return triples of info.\n",
        "'''\n",
        "from qwikidata.sparql import (get_subclasses_of_item,\n",
        "                              return_sparql_query_results)\n",
        "from datetime import datetime\n",
        "\n",
        "triples = []\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\"]\n",
        "count = 0\n",
        "for query in queries:\n",
        "  res = return_sparql_query_results(query)\n",
        "  #We want to save all the triples which are returned from the query\n",
        "  for item in res['results']['bindings']: \n",
        "    if count == 0:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.strftime(\"%B\")) + \" \" + str(dt.day) + \", \" + str(dt.year)))\n",
        "    elif count == 3:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.year)))\n",
        "    else:\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], item[res['head']['vars'][2]]['value']))\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu4nbvOG8ZD1"
      },
      "source": [
        "Let's examine the format of one of these returned triples, indicating that Kelis was born on August 21, 1979."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cBaIbmKABxN"
      },
      "source": [
        "print(triples[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbIC9V988fB"
      },
      "source": [
        "Now, let's iterate through our Wikipedia articles and align factual triples to sentences from our Wikipedia articles.\n",
        "\n",
        "You will add code to follow the entity alignment algorithm described in Figure 17.9 in SLP3.  Do not worry about the training component of the algorithm; this is covered in Deliverable 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPik_6-_IR4K"
      },
      "source": [
        "'''\n",
        "Now, we will iterate through and align the triples to sentences to create the dataset.\n",
        "\n",
        "This uses the algorithm from SLP3 Figure 17.9.\n",
        "'''\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_dataset(documents, triples, query_names):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  aligned_dataset = []\n",
        "\n",
        "  for document in tqdm(documents):\n",
        "    doc = nlp(documents[document])\n",
        "    sentences = list(doc.sents)\n",
        "    for sent in sentences:\n",
        "      sent = sent.text\n",
        "      for relation in query_names:\n",
        "        for triple in [t for t in triples if t[0] == relation]:\n",
        "          should_align = False\n",
        "          \n",
        "          #YOUR CODE HERE\n",
        "\n",
        "          if should_align:\n",
        "            #Let's mark the entities with a special prefix and join the multi-word ones with underscores as in SLP\n",
        "            formatted_sent = sent.replace(triple[1], \"_ent1_\" + \"_\".join(triple[1].split(\" \")))\n",
        "            formatted_sent = formatted_sent.replace(triple[2], \"_ent2_\" + \"_\".join(triple[2].split(\" \")))\n",
        "            aligned_dataset.append((formatted_sent, triple[0]))\n",
        "          \n",
        "  return aligned_dataset\n",
        "aligned_dataset = process_dataset(documents, triples, query_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJV56cXAhnn"
      },
      "source": [
        "Now let's look at our newly-aligned dataset, containing a small number of aligned triples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSGQoJdC5dk"
      },
      "source": [
        "aligned_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8v9zzyAoTm"
      },
      "source": [
        "We've now successfully used distant supervision to align sentences from Wikipedia articles to information triples from Wikidata.  Note that the dataset is not perfect, as it is done without human annotation.  This  process scales up without additional human effort, at the cost of more compute time.  For Deliverable 2, we will be providing a dataset created using Distant Supervision, as the compute-time required to create a sizable dataset is large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv5G5vQAMEk9"
      },
      "source": [
        "# Deliverable 2: Relation Prediction Model\n",
        "\n",
        "Now that we have the process to create an aligned dataset, let's train a CNN-based model to predict a relationship from the text spans.  Note that we will be using a different, larger dataset than the one you created in Deliverable 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYVNi56BlTg"
      },
      "source": [
        "train_dataset = \"./train_dataset.csv\"\n",
        "dev_dataset = \"./dev_dataset.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qbFLoebIR6d"
      },
      "source": [
        "'''\n",
        "Let's create a dictionary of relation types to define the classification output space\n",
        "For this deliverable we have an additional category, no_relation_found, which can be\n",
        "applied to sentences which do not align with a triple.\n",
        "'''\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\", \"no_relation_found\"]\n",
        "\n",
        "labels = {}\n",
        "count = 0 \n",
        "for query in query_names:\n",
        "  labels[query] = count\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tCtGJOMeMa"
      },
      "source": [
        "'''\n",
        "s1 and s2 define the position embeddings\n",
        "'''\n",
        "def get_batches(x, s1, s2, y, xType, batch_size=12):\n",
        "    batches_x=[]\n",
        "    batches_s1 = []\n",
        "    batches_s2 = []\n",
        "    batches_y=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        #import pdb; pdb.set_trace()\n",
        "        batches_x.append(xType(x[i:i+batch_size]))\n",
        "        batches_s1.append(xType(s1[i:i+batch_size]))\n",
        "        batches_s2.append(xType(s2[i:i+batch_size]))\n",
        "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
        "    \n",
        "    return batches_x,batches_s1, batches_s2, batches_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0QeNSfb3FdW"
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "MAX_DATA_LEN = 300\n",
        "\n",
        "data_lens = []\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0g9s_WVKM2l"
      },
      "source": [
        "This format_data() function is where you will add code to determine each word's position from m1 and m2.  As a reminder, we don't want to have negative values in m1_pos_list or m2_pos_list.  To address this, negative values will begin indexing after max_length (300).  For example, the position -10 would be stored as 310, the position -17 would be stored at 317, and so on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8m1yjWIR8v"
      },
      "source": [
        "import csv\n",
        "def format_data(filename, vocab, labels, max_length):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      filename: pointer to file holding the dataset we wish to process\n",
        "      vocab: GLoVE vocabulary file created from read_embeddings function\n",
        "      labels: dictionary mapping relationship name to integer index\n",
        "      max_length: maximum length of input\n",
        "    Returns:\n",
        "      data: Input sentences processed as glove embedding indices\n",
        "      data_m1: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the first entity (appended with _ent1_) with no negative values\n",
        "      data_m2: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the second entity (appended with _ent2_) with no negative values\n",
        "      data_labels:  Includes the integer label associated with each example in the dataset\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    data_m1 = []\n",
        "    data_m2 = []\n",
        "    file = open(filename)\n",
        "    csvreader = csv.reader(file, delimiter=',')\n",
        "\n",
        "    for line in csvreader:\n",
        "        sentence = line[0]\n",
        "        label = line[1]\n",
        "        \n",
        "        m1_pos_list = []\n",
        "        m2_pos_list = []\n",
        "        split_sentence = sentence.split(\" \")\n",
        "\n",
        "        #YOUR CODE HERE\n",
        "\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(sentence.lower()):\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "\n",
        "        #makes sure the example isn't too long for our model\n",
        "        if len(w_int) < 300:\n",
        "          w_int.extend([PAD_INDEX] * (max_length - len(w_int)))\n",
        "          data.append((w_int))\n",
        "          m1_pos_list.extend([max_length-1] * (max_length-len(m1_pos_list)))\n",
        "          data_m1.append(m1_pos_list)\n",
        "          m2_pos_list.extend([max_length*2-1] * (max_length-len(m2_pos_list)))\n",
        "          data_m2.append(m2_pos_list)\n",
        "          data_labels.append(labels[label])\n",
        "    return data, data_m1, data_m2, data_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1raXXcehISDr"
      },
      "source": [
        "class EntityCNNClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, params, pretrained_embeddings):\n",
        "      super().__init__()\n",
        "      self.seq_len = params[\"max_seq_len\"]\n",
        "      self.num_labels = params[\"label_length\"]\n",
        "      self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
        "\n",
        "      self.m1_embeddings = ...\n",
        "      self.m2_embeddings = ...\n",
        "\n",
        "      self.conv_2 = nn.Conv1d(82, 16, 2, 1)\n",
        "      self.pool_2 = nn.MaxPool1d(299,1)\n",
        "\n",
        "      self.fc = nn.Linear(16, self.num_labels)\n",
        "    \n",
        "   def forward(self, input, m1_pos_list, m2_pos_list): \n",
        "      x_word_emb = self.embeddings(input)\n",
        "\n",
        "      x_m1 = ...\n",
        "      x_m2 = ...\n",
        "\n",
        "      x = torch.cat((x_word_emb, x_m1, x_m2), 2)\n",
        "      x = x.permute(0, 2, 1)\n",
        "    \n",
        "      conv = self.conv_2(x)\n",
        "      conv = torch.tanh(conv)\n",
        "      conv = self.pool_2(conv)\n",
        "      conv = conv.view((conv.shape[0], -1))\n",
        "\n",
        "\n",
        "      self.out = self.fc(conv)\n",
        "      return self.out.squeeze()\n",
        "\n",
        "   def evaluate(self, x, s1, s2, y):\n",
        "      \n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for x, s1, s2, y in zip(x,s1, s2, y):\n",
        "          y_preds=self.forward(x, s1, s2)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKM9sDbNkQoj"
      },
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y = format_data(train_dataset, cnn_vocab, labels, 300)\n",
        "cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y = format_data(dev_dataset, cnn_vocab, labels, 300)\n",
        "cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY=get_batches(cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y, torch.LongTensor)\n",
        "cnn_devX, cnn_devS1, cnn_devS2, cnn_devY=get_batches(cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y, torch.LongTensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUzgVW-DTEyL"
      },
      "source": [
        "cnnmodel = EntityCNNClassifier(params={\"max_seq_len\": 100, \"label_length\": len(labels)}, pretrained_embeddings=embs)\n",
        "\n",
        "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=15\n",
        "best_dev_acc = 0.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnnmodel.train()\n",
        "\n",
        "    for x, s1, s2, y in zip(cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY):\n",
        "      y_pred = cnnmodel.forward(x, s1, s2)\n",
        "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
        "      losses.append(loss) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devS1, cnn_devS2, cnn_devY)\n",
        "    if epoch % 1 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "        if dev_accuracy > best_dev_acc:\n",
        "          torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
        "          best_dev_acc = dev_accuracy\n",
        "\n",
        "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MviKzbYqXqTg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}