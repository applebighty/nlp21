{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsOqhvi9s1-"
      },
      "source": [
        "# HW 5: Sequence Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PojCEPMrMEW8"
      },
      "source": [
        "In this homework, you will be using a bigram hidden Markov model (HMM) for sequence labelling. We explore two decoding algorithms: greedy (already implemented) and Viterbi (you will be implementing this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty-7ugq3Ypop"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kXha-KBBfvg"
      },
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW5/pos.train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfDjfeh_Yt3Z"
      },
      "source": [
        "def read_data(filename):\n",
        "    \"\"\"\n",
        "    Helper function to read in data from a file.\n",
        "    Outputs data points (X) and labels (Y), in two arrays of the same length.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "\n",
        "    current_sentence = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            if len(line) == 0:\n",
        "                continue\n",
        "            if line == '\\n':\n",
        "                if len(current_sentence) != 0:\n",
        "                    sentences.append(current_sentence)\n",
        "                    tags.append(current_tags)\n",
        "\n",
        "                current_sentence = []\n",
        "                current_tags = []\n",
        "            else:\n",
        "                columns = line.rstrip().split('\\t')\n",
        "                word = columns[0].lower()\n",
        "                tag = columns[1]\n",
        "\n",
        "                current_sentence.append(word)\n",
        "                current_tags.append(tag)\n",
        "\n",
        "        return sentences, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CszpS8NkYvMP"
      },
      "source": [
        "def create_vocab(train_x, train_y):\n",
        "    \"\"\"\n",
        "    Outputs two dictionaries:\n",
        "    (1) mapping each vocab word to an index of 0 thru LEN(TRAIN_X)-1\n",
        "    (2) mapping each tag to an index of 0 thru LEN(TRAIN_Y)-1\n",
        "    \"\"\"\n",
        "    x_vocab = {}\n",
        "    y_vocab = {}\n",
        "    for xs, ys in zip(train_x, train_y):\n",
        "        for x, y in zip(xs, ys):\n",
        "            if x not in x_vocab:\n",
        "                x_vocab[x] = len(x_vocab)\n",
        "            if y not in y_vocab:\n",
        "                y_vocab[y] = len(y_vocab)\n",
        "\n",
        "    y_vocab[\"START\"] = len(y_vocab)\n",
        "    y_vocab[\"END\"] = len(y_vocab)\n",
        "\n",
        "    return x_vocab, y_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58rvZWe9YwrP"
      },
      "source": [
        "def estimate_parameters(x_vocab, y_vocab, train_x, train_y):\n",
        "    \"\"\"\n",
        "    Returns a transitions matrix and emissions matrix with probabilities\n",
        "    given in LOG space.\n",
        "    transitions[s1, s2]: log probability of observing label s2 after label s1.\n",
        "    emissions[s, t]: log probability of observing token t with label s.\n",
        "    \"\"\"\n",
        "    transitions = np.zeros((len(y_vocab)-1, len(y_vocab)))\n",
        "    emissions = np.zeros((len(y_vocab)-2, len(x_vocab)))\n",
        "\n",
        "    for xs, ys in zip(train_x, train_y):\n",
        "        prev = y_vocab[\"START\"]\n",
        "        for i in range(len(ys)):\n",
        "            y_idx = y_vocab[ys[i]]\n",
        "            x_idx = x_vocab[xs[i]]\n",
        "            transitions[prev][y_idx] += 1\n",
        "            prev = y_idx\n",
        "            emissions[y_idx][x_idx] += 1\n",
        "        transitions[prev][y_vocab[\"END\"]] += 1\n",
        "\n",
        "    # normalize each row by its total\n",
        "    transitions = transitions/np.sum(transitions, axis=1, keepdims=True)\n",
        "    emissions = emissions/np.sum(emissions, axis=1, keepdims=True)\n",
        "\n",
        "    # let's work in log space (adding instead of multiplying)\n",
        "    transitions = np.log(transitions)\n",
        "    emissions = np.log(emissions)\n",
        "\n",
        "    return transitions, emissions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMhN7dbgY3kY"
      },
      "source": [
        "train_x, train_y = read_data(\"pos.train\")\n",
        "\n",
        "x_vocab, y_vocab = create_vocab(train_x, train_y)\n",
        "transitions, emissions = estimate_parameters(x_vocab, y_vocab, train_x, train_y)\n",
        "# You might see a \"RuntimeWarning: divide by zero encountered in log\" error â€” that's ok!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4AcNEs3G76o"
      },
      "source": [
        "Run the following cell to understand the sizes and dimensions of the vocabulary, POS tag labels, transitions, and emissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSE0b2xS_xJN"
      },
      "source": [
        "print(\"# of words in vocabulary: %d\" % len(x_vocab))\n",
        "print(\"# of POS tag labels (including START and END): %d\" % len(y_vocab))\n",
        "print(\"dimension of transitions: %s\" % (transitions.shape,))\n",
        "print(\"dimension of emissions: %s\" % (emissions.shape,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAVGKq-UIMin"
      },
      "source": [
        "The \"START\" and \"END\" tags are considered \"non-emitting\" states and therefore have no corresponding entries in the emissions matrix. (In other words, there's no probability of observing a word token labeld \"START\" or \"END\" since those are special labels.) According to the `y_vocab` map, `START` and `END` correspond to indices 50 and 51 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3QJsGKU_ASR"
      },
      "source": [
        "## Greedy Decode\n",
        "This decoding algorithm proceeds left to right, committing to the best tag for each time step (given the previous tag label and current time step observation). You do not need to modify anything in the `greedy_decode` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLvNZk-QYy1v"
      },
      "source": [
        "def greedy_decode(transitions, emissions, y_vocab, x_vocab, sequence):\n",
        "    prev = y_vocab[\"START\"]\n",
        "    best_prediction = []\n",
        "    for x in sequence:\n",
        "        maxlogprob = -100000000\n",
        "        best_y = None\n",
        "        for y in y_vocab:\n",
        "            if y != \"END\" and y != \"START\":\n",
        "                logprob = transitions[prev][y_vocab[y]] + emissions[y_vocab[y]][x_vocab[x]]\n",
        "                if logprob > maxlogprob:\n",
        "                    maxlogprob = logprob\n",
        "                    best_y = y\n",
        "        prev = y_vocab[best_y]\n",
        "        best_prediction.append(best_y)\n",
        "\n",
        "    return ' '.join(best_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiD0ydD4nAxa"
      },
      "source": [
        "The following cell runs the `greedy_decode` algorithm on the sentence \"fruit flies like a banana\". Take a look at the sequence labels it predicts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ysgvDbAawSg"
      },
      "source": [
        "greedy_prediction = greedy_decode(transitions, emissions, y_vocab, x_vocab, [\"fruit\", \"flies\", \"like\", \"a\", \"banana\"])\n",
        "print(greedy_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVBBfKiEbQ6"
      },
      "source": [
        "## Viterbi Decode\n",
        "Your task is to implement the Viterbi algorithm to find the optimal sequence of POS tag labels for a sentence or word sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRYfE3NNKUsL"
      },
      "source": [
        "def viterbi_decode(transitions, emissions, y_vocab, x_vocab, sequence):\n",
        "    \"\"\"\n",
        "    transitions : a matrix where the entry transitions[s1, s2] is the log\n",
        "                  probability of observing label s2 after label s1 in a sequence.\n",
        "    emissions   : a matrix where the entry emissions[s, t] is the log \n",
        "                  probability of observing token t with label s.\n",
        "    y_vocab     : a dictionary mapping each POS tag label to an index of 0 thru\n",
        "                  the number of POS tag labels.\n",
        "    x_vocab     : a dictionary mapping each vocab word to an index of 0 thru the\n",
        "                  number of vocab words.\n",
        "    sequence    : a list of (string) words/tokens.\n",
        "\n",
        "    Returns a string with the POS tag labels (separated by spaces) for the words\n",
        "    in the sequence.\n",
        "    For example,\n",
        "\n",
        "    >>> viterbi_decode(transitions, emissions, y_vocab, x_vocab, [\"fruit\", \"flies\", \"like\", \"a\", \"banana\"])\n",
        "    \"NN VBZ IN DT NN\"\n",
        "\n",
        "    Do not change the order or naming of the input parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIETy-TeCFIq"
      },
      "source": [
        "The following cell runs the `viterbi_decode` algorithm on the sentence \"fruit flies like a banana\". For this particular sequence, the Viterbi-predicted sequence labels should be the same as the ones predicted by the greedy decode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0QXzzGvCYF4"
      },
      "source": [
        "viterbi_prediction = viterbi_decode(transitions, emissions, y_vocab, x_vocab, [\"fruit\", \"flies\", \"like\", \"a\", \"banana\"])\n",
        "print(viterbi_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW3h-cEqIF2W"
      },
      "source": [
        "## Examples: Greedy vs Viterbi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkL3-fSUGNua"
      },
      "source": [
        "def create_compare_fn(transitions, emissions, y_vocab, x_vocab):\n",
        "    \"\"\"\n",
        "    Helper function to compare results from the greedy decode and viterbi decode.\n",
        "    You do not have to modify this function.\n",
        "    \"\"\"\n",
        "    def compare_greedy_and_viterbi(sentence):\n",
        "        sequence = sentence.lower().split(\" \")\n",
        "        greedy_pred = greedy_decode(transitions, emissions, y_vocab, x_vocab, sequence)\n",
        "        viterbi_pred = viterbi_decode(transitions, emissions, y_vocab, x_vocab, sequence)\n",
        "        return greedy_pred, viterbi_pred\n",
        "    return compare_greedy_and_viterbi\n",
        "\n",
        "compare_greedy_and_viterbi = create_compare_fn(transitions, emissions, y_vocab, x_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KfxCxJaByVv"
      },
      "source": [
        "The following cells run the greedy algorithm and Viterbi algorithm on various sentences. Take a look at the sequence labels it predicts for both. If your Viterbi implementation is correct, the greedy and Viterbi sequence labels should be different for all of the following examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvCMswTJlFAZ"
      },
      "source": [
        "g, v = compare_greedy_and_viterbi(\"Bear the feeling .\")\n",
        "print(g)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KggDnQuDG9x2"
      },
      "source": [
        "g, v = compare_greedy_and_viterbi(\"Present an object .\")\n",
        "print(g)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yesgXDNIAUf"
      },
      "source": [
        "g, v = compare_greedy_and_viterbi(\"The judge objects to the suspect speaking .\")\n",
        "print(g)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNBMSBvyC_FG"
      },
      "source": [
        "Let's look at a garden path sentence (similar to the one from class, though \"raced\" is not in our vocabulary in this dataset). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GeVMNA9JptY"
      },
      "source": [
        "g, v = compare_greedy_and_viterbi(\"The horse led past the barn fell .\")\n",
        "print(g)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGhyZb1Bm40"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}